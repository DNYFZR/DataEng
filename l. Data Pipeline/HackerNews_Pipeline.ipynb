{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "__Hacker News Pipeline__\n",
    "\n",
    "We have built a data pipeline that schedules our tasks.\n",
    "\n",
    "The data we will use comes from a Hacker News (HN) API, returning JSON data of the top stories in 2014.\n",
    "\n",
    "Each post has a set of keys, but we will deal only with the following keys:\n",
    "- created_at: A timestamp of the story's creation time.\n",
    "- created_at_i: A unix epoch timestamp.\n",
    "- url: The URL of the story link.\n",
    "- objectID: The ID of the story.\n",
    "- author: The story's author (username on HN).\n",
    "- points: The number of upvotes the story had.\n",
    "- title: The headline of the post.\n",
    "- num_comments: The number of a comments a post has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "import json, csv, io, string, datetime as dt\n",
    "\n",
    "from pipeline import Pipeline, build_csv\n",
    "from stop_words import stop_words\n",
    "\n",
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data\n",
    "#  Load data from JSON file\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json', 'r') as f:\n",
    "        raw = json.load(f)\n",
    "        data = raw['stories']\n",
    "    return data\n",
    "\n",
    "#  Filter data\n",
    "@pipeline.task(depends_on = file_to_json)\n",
    "def filter_data(data):\n",
    "    def popular(item):\n",
    "        return (item['points'] > 50 \n",
    "                and item['num_comments'] > 1 \n",
    "                and not item['title'].startswith('ASK HN')\n",
    "               )\n",
    "    return (item for item in data if popular(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Data\n",
    "#  Convert JSON data to CSV\n",
    "@pipeline.task(depends_on = filter_data)\n",
    "def json_to_csv(data):\n",
    "    lines = list()\n",
    "    for item in data:\n",
    "        lines.append((item['objectID'], \n",
    "                      dt.datetime.strptime(item['created_at'], '%Y-%m-%dT%H:%M:%SZ'),\n",
    "                      item['url'], item['points'], item['title']\n",
    "                     ))\n",
    "        \n",
    "    file = build_csv(lines, \n",
    "                     header = ['objectID', 'created_at', 'url', 'points', 'title'],\n",
    "                     file = io.StringIO())\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Isolate title data\n",
    "@pipeline.task(depends_on = json_to_csv)\n",
    "def extract_titles(file):\n",
    "    reader = csv.reader(file)\n",
    "    header = next(reader)\n",
    "    id_num = header.index('title')\n",
    "    return (i[id_num] for i in reader)\n",
    "\n",
    "#  Standardise title data\n",
    "@pipeline.task(depends_on = extract_titles)\n",
    "def clean_titles(titles):\n",
    "    titles = [t.lower() for t in titles]\n",
    "    for p in string.punctuation:\n",
    "        titles = [t.replace(p, '') for t in titles]\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Build key - value store of word frequencies\n",
    "@pipeline.task(depends_on = clean_titles)\n",
    "def build_dictionary(titles):\n",
    "    word_freq = {}\n",
    "    for t in titles:\n",
    "        for i in t.split(' '):\n",
    "            if len(i) == 0 or i in stop_words:\n",
    "                pass            \n",
    "            else:\n",
    "                if i not in word_freq.keys():\n",
    "                    word_freq[i] = 1\n",
    "\n",
    "                word_freq[i] += 1\n",
    "    \n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange frequency table\n",
    "@pipeline.task(depends_on = build_dictionary)\n",
    "def top_entries(word_freq, no_entries = 100):\n",
    "    sorted_items = sorted(word_freq.items(),\n",
    "                          key=lambda x:x[1], \n",
    "                          reverse=True)\n",
    "    \n",
    "    return sorted_items[:no_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Entries\n",
      "new - 186\n",
      "google - 168\n",
      "ask - 127\n",
      "bitcoin - 103\n",
      "open - 96\n",
      "programming - 93\n",
      "web - 90\n",
      "data - 87\n",
      "video - 80\n",
      "python - 76\n",
      "code - 75\n",
      "facebook - 72\n",
      "released - 72\n",
      "using - 71\n",
      "source - 69\n",
      "2013 - 66\n",
      "2014 - 66\n",
      "free - 66\n",
      "javascript - 66\n",
      "game - 65\n",
      "internet - 63\n",
      "c - 61\n",
      "work - 60\n",
      "microsoft - 60\n",
      "linux - 59\n",
      "app - 58\n",
      "pdf - 56\n",
      "language - 55\n",
      "software - 55\n",
      "use - 54\n",
      "startup - 53\n",
      "make - 52\n",
      "apple - 51\n",
      "time - 50\n",
      "yc - 49\n",
      "security - 49\n",
      "nsa - 46\n",
      "github - 46\n",
      "windows - 45\n",
      "like - 45\n",
      "project - 43\n",
      "way - 43\n",
      "world - 42\n",
      "users - 41\n",
      "developer - 41\n",
      "1 - 41\n",
      "computer - 41\n",
      "heartbleed - 41\n",
      "dont - 39\n",
      "git - 38\n",
      "design - 38\n",
      "ios - 38\n",
      "os - 37\n",
      "twitter - 37\n",
      "ceo - 37\n",
      "online - 37\n",
      "vs - 37\n",
      "big - 37\n",
      "life - 37\n",
      "day - 36\n",
      "android - 35\n",
      "years - 35\n",
      "apps - 35\n",
      "best - 35\n",
      "simple - 34\n",
      "mt - 34\n",
      "court - 34\n",
      "firefox - 33\n",
      "guide - 33\n",
      "learning - 33\n",
      "gox - 33\n",
      "site - 33\n",
      "api - 33\n",
      "says - 33\n",
      "browser - 33\n",
      "server - 32\n",
      "fast - 32\n",
      "problem - 32\n",
      "mozilla - 32\n",
      "engine - 32\n",
      "introducing - 31\n",
      "does - 31\n",
      "amazon - 31\n",
      "better - 31\n",
      "year - 31\n",
      "text - 31\n",
      "support - 30\n",
      "stop - 30\n",
      "tech - 30\n",
      "built - 30\n",
      "million - 30\n",
      "money - 30\n",
      "people - 30\n",
      "3 - 29\n",
      "developers - 29\n",
      "development - 29\n",
      "did - 29\n",
      "help - 29\n",
      "learn - 29\n",
      "billion - 28\n"
     ]
    }
   ],
   "source": [
    "# Test implementation\n",
    "test = pipeline.run()\n",
    "print('Top Entries')\n",
    "for i in test[top_entries]:\n",
    "    print(i[0], '-', i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Closing remarks__\n",
    "\n",
    "1. The data on HackerNews posts has been proceessed using the pipeline for task scheduling.\n",
    "2. The data has been cleaned to standardise the word format, as well as skipping stop words and blank entries. \n",
    "3. The frequency of each word in the post titles has been extracted into a key - value store.\n",
    "4. The top 100 words in the key - value store have been extracted and displayed in a readable format. \n",
    "\n",
    "The final result has some interesting keywords. There were terms like bitcoin, heartbleed (the 2014 hack), and many others. \n",
    "\n",
    "Now that we have created the pipeline, there are additional tasks we could perform with the data:\n",
    "\n",
    "- Rewrite the Pipeline class' output to save a file of the output for each task. This will allow you to \"checkpoint\" tasks so they don't have to be run twice.\n",
    "- Use the nltk package for more advanced natural language processing tasks.\n",
    "- Convert to a CSV before filtering, so you can keep all the stories from 2014 in a raw file.\n",
    "- Fetch the data from Hacker News directly from a JSON API. Instead of reading from the file we gave, you can perform additional data processing using newer data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
